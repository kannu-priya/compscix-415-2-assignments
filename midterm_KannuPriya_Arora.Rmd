---
title: "Compscix-415-2-Mid Term/Assignment 5"
author: "Kannu Priya Arora"
date: "March 3, 2018"
output:
  html_document :
    toc : TRUE
---

```{r load_packages, include=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
```

### *Git and GitHub*

Below is the link of my github repository: 
https://github.com/kannu-priya/compscix-415-2-assignments.git


###SECTION 1.  RStudio and R Markdown

  1.  *Use markdown headers in your document to clearly separate each midterm question and add a table of contents to your document*

Kindly refer above the table of contents 

***

###SECTION 2.  The tidyverse packages 
*By now you’ve used at least five different packages from the tidyverse for plotting, data munging, reshaping data, importing/exporting data, and using tibbles (the tibble package is used for this without you even realizing it’s there).*

  1.  *Can you name which package is associated with each task below?*
  
  | Task | Package Name |
  |:-------------|:-------|
  | Plotting | ggplot2 |
  | Data munging/wrangling | dplyr, tidyr |
  | Reshaping (spreading and gathering) data | tidyr |
  | Importing/exporting data | readr |
  
  
  
  2.  *Now can you name two functions that you’ve used from each package that you listed above for these tasks?*

  | Task | Package Name | Functions |
  |:--------------|:-------|:----------------|
  | Plotting | ggplot2 | geom_FUNCTION() - eg: geom_point(), stat_count(), facet_grid() |
  | Data munging/wrangling | dplyr |  filter(), summarise() |
  | Reshaping data | tidyr | spread(), gather() |
  | Importing/exporting data | readr | read_csv(), write_rds() |

***

###SECTION 3.  R Basics

  1.  *Fix this code with the fewest number of changes possible so it works:*
*My_data.name___is.too00ooLong! <- c( 1 , 2   , 3 )*

  Letter "!" is not allowed in naming convention. Just removing that will make the code work

```{r}
(My_data.name___is.too00ooLong <- c( 1 , 2   , 3 ))

```


  2.  *Fix this code so it works:*
  *my_string <- C('has', 'an', 'error', 'in', 'it)*

  Changes - function "c" should be in small case and last value "it" quotes were not complete

```{r}
(my_string <- c('has', 'an', 'error', 'in', 'it'))
```

  3.  *Look at the code below and comment on what happened to the values in the vector.*
  my_vector <- c(1, 2, '3', '4', 5)
  my_vector
  ## [1] "1" "2" "3" "4" "5"

  All values are converted to character type and final vector returned is also character. If the single quotes are removed from number 3 and 4, then the values will remain as numeric and final vector is double. 

```{r}
my_vector <- c(1, 2, '3', '4', 5)
typeof(my_vector)
```

***

###SECTION 4.  Data import/export

  1.  *Download the rail_trail.txt file from Canvas (in the Midterm Exam section here) and successfully import it into R. Prove that it was imported successfully by including your import code and taking a glimpse of the result.*


```{r}
rail_file <- read_delim(file = 'D:/Kannu Priya/DataScience/compscix-415-2-assignments/rail_trail.txt', "|")
glimpse(rail_file)
```


  2.  *Export the file into an R-specific format and name it “rail_trail.rds”. Make sure you define the path correctly so that you know where it gets saved. Then reload the file. Include your export and import code and take another glimpse.*

```{r}
write_rds(rail_file, "rail_file.rds")
glimpse(read_rds("rail_file.rds"))
```

***

###SECTION 5.  Visualization

  1.  *Critique this graphic: give only three examples of what is wrong with this graphic. Be concise.*

    Critique:
    a.  With use of multiple pie charts (faceting), it is left to viewers to read through each % figures and find out the variation of trend.
    b.  The scale of pie charts looks extreme and percentages for each row don't add up to 100%.
    c.  Aesthetically, the black color is used to represent all age groups and then in same order two different color are used to show gender variation. This can be confusing to some people. Further, black color is not adding any value to changing numbers and the color is not appealing. 
    

    
  2.  *Reproduce this graphic using the diamonds data set.*

```{r}
  ggplot(diamonds)+
  geom_boxplot(mapping = aes(x=cut, y=carat, fill = color), position = "identity") +
  coord_flip() + 
  ylab("CARAT OF DIAMOND")+
  xlab("CUT OF DIAMOND")
```


  3.  *The previous graphic is not very useful. We can make it much more useful by changing one thing about it. Make the change and plot it again.*

Rearranging the cut of diamonds around their median weight will make the graph more useful. Each box plot has been split around color, thus there are multiple median lines in the box for each color.Viewer cannot determine on an average which cut has highest or lowest carat size.

In the below modified version, the Cut axis has been reordered around overall median carat size. We now clearly know how the median weights vary across different type of cuts with Fair having highest weight and Ideal the lowest.


```{r}
  ggplot(diamonds)+
  geom_boxplot(mapping = aes(x=reorder (cut,carat, FUN = median), y=carat, fill = color), position = "identity") +
  coord_flip() + 
  ylab("CARAT OF DIAMOND")+
  xlab("CUT OF DIAMOND")
```


***

###SECTION 6.  Data munging and wrangling

  1.   *Is this data “tidy”? If yes, leave it alone and go to the next problem. If no, make it tidy. Note: this data set is called table2 and is available in the tidyverse package. It should be ready for you to use after you’ve loaded the tidyverse package.*

the data is not tidy - country in a year observation is distributed in 2 rows. to make the data tidy, we can use spread function

```{r}
spread(table2, key=type, value=count)
```



  2.  *Create a new column in the diamonds data set called price_per_carat that shows the price of each diamond per carat (hint: divide). Only show me the code, not the output.*

Code: 
diamonds %>% mutate(price_per_carat = price / carat)

```{r include = FALSE}
diamonds %>% mutate(price_per_carat = price / carat)
```


  3.  *For each cut of diamond in the diamonds data set, how many diamonds, and what proportion, have a price > 10000 and a carat < 1.5? There are several ways to get to an answer, but your solution must use the data wrangling verbs from the tidyverse in order to get credit.* 


```{r}
(diamond_summary <- diamonds %>% 
    group_by(cut) %>%
   summarise(total = n(),
             filtered_count = sum(price>10000 & carat<1.5),
             proportion = mean(price>10000 & carat<1.5))
)
```


  -   *Do the results make sense? Why?*
  
  The table above shows proportion of diamonds that are highly priced (>10000) and simultaneously very low in Carat quality (<1.5). This does not make sense in itself but if we look at how this proportion varies with Cut then that explains the higher price. As we can observe, the proportion of high priced diamonds with low carat increases with quality of diamond cut. Better the diamond cut, higher it is priced (irrespective of the Carat weight) 
  
  -   *Do we need to be wary of any of these numbers? Why?*
  
  I will be cautious of first two categories - Fair & Good type of cut, priced at greater than 10000 and less than 1.5 Carat. The observations do not make sense and can be some data error. Statistically as well, since the number of counts are very low, a small change in data will have high impact on the proportion.

***

###SECTION 7.  EDA
*Take a look at the txhousing data set that is included with the ggplot2 package and answer these questions:*

  1.  *During what time period is this data from?*
  
  Data observations are from Jan 2000 to Jul 2015
  
```{r include = FALSE}
txhousing %>% 
  group_by(month, year)%>%
  arrange(year, month)
```

  2.  *How many cities are represented?*

The data set represents 46 cities from Texas state 

  3.  *Which city, month and year had the highest number of sales?*

Houston had highest sales in 07/2015 (July 2015)

Individually, just looking at city, Houston had highest overall sales.
Similarly, Year 2006 had highest overall sales and month of June has highest aggregated sales

```{r include = FALSE}
(txhousing %>%
  group_by(city, year, month)%>%
   summarise(Total = sum(sales, na.rm = TRUE))%>%
   arrange(desc(Total))
)
```

  
  4.  *What kind of relationship do you think exists between the number of listings and the number of sales? Check your assumption and show your work.*

There is positive correlation between number of listings and sales - higher the active listings, the number of sales are also higher. The rate at which sales increase with every increase in listings is high initially (steep curve) but slows down gradually at 30000 listings and more
  

```{r}
ggplot(txhousing, mapping = aes(listings, sales)) +
         geom_smooth()
```


  5.  *What proportion of sales is missing for each city?*

Highest missing sales are for South Padre Island (missing proportion of 0.6 i.e. 62% missing values)

```{r}
txhousing %>%
  group_by(city)%>%
  summarise(missing = mean(is.na(sales)))%>%
  arrange(desc(missing))
```


  *6.  Looking at only the cities and months with greater than 500 sales:* 
  
  * *Are the distributions of the median sales price (column name median), when grouped by city, different? The same? Show your work*

  
  The distribution of median sale prices varies across cities. As we can see in first graph, the median sale prices tend to peak in summer and then towards year end. The graph further below shows the city wise variation of median sale prices - the cities are in descending order with Collin County being the highest in highest median sale price followed by Austin.


```{r}
txhousing_500 <-  txhousing %>% 
  group_by(city, month)%>%
  filter(sales>500) %>%
  rename(Median = median)

txhousing_500 %>% 
  ggplot() +
  geom_smooth(mapping = aes(x= month, y= Median))

ggplot(txhousing_500)+
  geom_boxplot(mapping = aes(x = reorder(city, Median, FUN = median), y=Median)) +
  coord_flip() +
  xlab("Cities")
  
```

  
  * *Any cities that stand out that you’d want to investigate further?*

From the graph, Corpus Christi looks different from other cities with very small range of box plot. Further investigation shows the city has only 5 months from summer of 2005 and 2006 when sales exceeded 500 and the median price remained similar during that time. Looking at how the overall sales varied for the city, we can see below the city real estate had a peak in 2005 - 2006. Even though the median price has been increasing over years, but since the greater than 500 data set was from very close time period, we saw a stable median price


```{r}
txhousing %>% 
  filter(city == 'Corpus Christi') %>%
  ggplot(mapping= aes(x=year, y=sales))+
  geom_smooth()
  
```



  * *Why might we want to filter out all cities and months with sales less than 500?*

The order of variation in median sale price would be very high if all sales were included. Further, the list of cities will get very large. It will be very difficult to do a focused meaningful analysis through visualization on such large categorical data-set.  

***